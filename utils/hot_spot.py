from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import jieba
from typing import List, Dict, Tuple, Optional, TYPE_CHECKING
from datetime import datetime, timedelta

if TYPE_CHECKING:
    from sqlalchemy.orm import Session


class MunicipalHotspotRanker:
    """
    å¸‚æ”¿åŸºç¡€è®¾æ–½é—®é¢˜çƒ­åº¦åˆ†æå™¨
    åŠŸèƒ½ï¼šè¯†åˆ«ç›¸ä¼¼é—®é¢˜ã€èšç±»å½’é›†ã€ç”Ÿæˆçƒ­åº¦æ’è¡Œæ¦œ
    """
    
    def __init__(self, similarity_threshold: float = 0.6, db_session: Optional['Session'] = None):
        """
        åˆå§‹åŒ–çƒ­åº¦åˆ†æå™¨
        :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œå€¼è¶Šå¤§è¦æ±‚è¶Šç›¸ä¼¼
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼Œå¦‚æœæä¾›åˆ™ä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®
        """
        # ä¸ºäº†æ”¯æŒåŸºäºæ—¶é—´çš„çƒ­åº¦è®¡ç®—ï¼Œä½¿ç”¨å¹¶è¡Œåˆ—è¡¨å­˜å‚¨æ–‡æœ¬ä¸æ—¶é—´
        self.report_texts: List[str] = []  # æŠ¥å‘Šæ–‡æœ¬ï¼Œç”¨äºå‘é‡åŒ–
        self.report_times: List[datetime] = []  # å¯¹åº”çš„æ—¶é—´æˆ³
        self.report_cluster_map = {}  # æŠ¥å‘Šç´¢å¼•åˆ°èšç±»IDçš„æ˜ å°„
        self.clusters = {}  # èšç±»ä¿¡æ¯ {cluster_id: {'representative': ä»£è¡¨æ–‡æœ¬, 'count': æ•°é‡, 'reports': [ç´¢å¼•åˆ—è¡¨]}}
        self.cluster_counter = 0  # èšç±»IDè®¡æ•°å™¨
        
        # ä¸­æ–‡åˆ†è¯å™¨é…ç½®
        self.tokenizer = lambda text: ' '.join(jieba.cut(text))
        
        # TF-IDFå‘é‡åŒ–å™¨ï¼Œæ”¯æŒä¸­æ–‡
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.tokenizer,
            token_pattern=None,
            lowercase=False,
            max_features=5000,
            ngram_range=(1, 2)  # æ”¯æŒ1-gramå’Œ2-gram
        )
        self.tfidf_matrix = None
        self.similarity_threshold = similarity_threshold
        
        # å¦‚æœæä¾›äº†æ•°æ®åº“ä¼šè¯ï¼Œä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®
        if db_session is not None:
            self.load_from_database(db_session)

    def add_report(self, text: str, report_time: Optional[datetime] = None) -> int:
        """
        æ·»åŠ ä¸€æ¡æ–°çš„å¸‚æ”¿é—®é¢˜ä¸ŠæŠ¥è®°å½•ï¼Œå¹¶è‡ªåŠ¨è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…å’Œèšç±»
        :param text: ç”¨æˆ·ä¸ŠæŠ¥çš„é—®é¢˜æ–‡æœ¬
        :param report_time: æŠ¥å‘Šæ—¶é—´ï¼ˆé»˜è®¤ç°åœ¨ï¼‰
        :return: æ–°æŠ¥å‘Šçš„ç´¢å¼•
        """
        if not text or not text.strip():
            raise ValueError("é—®é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        text = text.strip()
        if report_time is None:
            report_time = datetime.now()
        
        report_idx = len(self.report_texts)
        self.report_texts.append(text)
        self.report_times.append(report_time)
        
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¡æŠ¥å‘Šï¼Œç›´æ¥åˆ›å»ºæ–°èšç±»
        if report_idx == 0:
            self._create_new_cluster(report_idx, text)
            self.tfidf_matrix = self.vectorizer.fit_transform([text])
        else:
            # å…ˆä¸´æ—¶å‘é‡åŒ–æ–°æ–‡æœ¬ï¼ˆä½¿ç”¨å·²æœ‰vocabularyï¼‰ï¼Œç”¨äºç›¸ä¼¼åº¦åŒ¹é…
            # å¦‚æœvectorizerè¿˜æœªfitï¼Œåˆ™å…ˆfitæ‰€æœ‰å·²æœ‰æŠ¥å‘Š
            if self.tfidf_matrix is None or self.tfidf_matrix.shape[0] == 0:
                self._rebuild_vectorizer()
            
            # å°è¯•åŒ¹é…åˆ°ç°æœ‰èšç±»
            matched_cluster_id = self._find_matching_cluster(text)
            
            if matched_cluster_id is None:
                # æ²¡æœ‰åŒ¹é…åˆ°ï¼Œåˆ›å»ºæ–°èšç±»
                self._create_new_cluster(report_idx, text)
            else:
                # åŒ¹é…åˆ°ç°æœ‰èšç±»ï¼Œæ·»åŠ åˆ°è¯¥èšç±»
                self._add_to_cluster(report_idx, matched_cluster_id)
            
            # é‡æ–°æ„å»ºå‘é‡çŸ©é˜µï¼ˆç¡®ä¿vocabularyåŒ…å«æ‰€æœ‰æ–°è¯ï¼‰
            self._rebuild_vectorizer()
        
        return report_idx

    def _create_new_cluster(self, report_idx: int, text: str) -> int:
        """åˆ›å»ºæ–°çš„èšç±»"""
        cluster_id = self.cluster_counter
        self.cluster_counter += 1
        self.report_cluster_map[report_idx] = cluster_id
        self.clusters[cluster_id] = {
            'representative': text,
            'count': 1,
            'reports': [report_idx]
        }
        return cluster_id

    def compute_heat_for_cluster(self, cluster_id: int, now: Optional[datetime] = None) -> float:
        """
        æ ¹æ®è§„åˆ™è®¡ç®—æŒ‡å®šèšç±»çš„çƒ­åº¦å€¼
        è§„åˆ™ï¼š
          åŸºç¡€çƒ­åº¦ï¼šç´§æ€¥ç±»10ï¼Œå¿«é€Ÿå¤„ç†5ï¼Œå¸¸è§„0ï¼ˆé€šè¿‡severityLevelä¼ å…¥æˆ–åœ¨å¤–éƒ¨æ˜ å°„ï¼‰
          ä¸ŠæŠ¥æ¬¡æ•°å¾—åˆ†ï¼šæ¯ä¸€æ¬¡ç‹¬ç«‹ä¸ŠæŠ¥ +2
          é›†ä¸­ä¸ŠæŠ¥åŠ æˆï¼š1å°æ—¶å†…ä¸ŠæŠ¥æ¬¡æ•°>3ï¼Œæ¯å¤šä¸€æ¬¡ä¸ŠæŠ¥ +1
          æ—¶é—´è¡°å‡ï¼šæ¯å°æ—¶ -0.1ï¼Œä½†æœ€ä½ä¸ä½äº0
        :param cluster_id: èšç±»ID
        :param now: è®¡ç®—å‚è€ƒæ—¶é—´ï¼ˆé»˜è®¤ç°åœ¨ï¼‰
        :return: è®¡ç®—åçš„çƒ­åº¦ï¼ˆæµ®ç‚¹æ•°ï¼Œåº•çº¿ä¸º0ï¼‰
        """
        if now is None:
            now = datetime.now()
        if cluster_id not in self.clusters:
            return 0.0
        cluster = self.clusters[cluster_id]
        report_indices = cluster.get('reports', [])

        # ä¸ŠæŠ¥æ¬¡æ•°å¾—åˆ†
        report_count = len(report_indices)
        report_score = report_count * 2.0

        # è®¡ç®—é›†ä¸­ä¸ŠæŠ¥ï¼ˆè¿‡å»1å°æ—¶å†…ä¸ŠæŠ¥æ¬¡æ•°ï¼‰
        one_hour_ago = now - timedelta(hours=1)
        recent_count = 0
        for idx in report_indices:
            if idx < len(self.report_times) and self.report_times[idx] >= one_hour_ago:
                recent_count += 1
        concentrated_bonus = 0.0
        if recent_count > 3:
            concentrated_bonus = float(recent_count - 3) * 1.0

        # æ—¶é—´è¡°å‡ï¼šå–æœ€æ—©æŠ¥å‘Šæ—¶é—´åˆ°ç°åœ¨çš„å°æ—¶å·®ä½œä¸ºæ€»æ—¶å·®
        # ä¹Ÿå¯ä»¥ä½¿ç”¨èšç±»ç¬¬ä¸€ä¸ªæŠ¥å‘Šæ—¶é—´æˆ–å¹³å‡æ—¶é—´ï¼Œè¿™é‡Œä½¿ç”¨æœ€æ—©æ—¶é—´
        earliest_time = None
        for idx in report_indices:
            if idx < len(self.report_times):
                t = self.report_times[idx]
                if earliest_time is None or t < earliest_time:
                    earliest_time = t
        if earliest_time is None:
            hours_diff = 0
        else:
            hours_diff = max(0, (now - earliest_time).total_seconds() / 3600.0)
        time_decay = hours_diff * 0.1

        # åŸºç¡€çƒ­åº¦é¡¹ä¿ç•™ä¸ºå¤–éƒ¨ä¼ å…¥ï¼ˆç”±è·¯ç”±ç«¯æ•´åˆseverityLevelï¼‰ï¼Œå› æ­¤è¿™é‡Œåªè¿”å›é™„åŠ å€¼
        # æœ€ç»ˆçƒ­åº¦è®¡ç®—ç”±å¤–éƒ¨æ±‡æ€»ï¼šåŸºç¡€çƒ­åº¦ + report_score + concentrated_bonus - time_decay
        heat = max(0.0, report_score + concentrated_bonus - time_decay)
        return heat
    def _add_to_cluster(self, report_idx: int, cluster_id: int):
        """å°†æŠ¥å‘Šæ·»åŠ åˆ°æŒ‡å®šèšç±»"""
        self.report_cluster_map[report_idx] = cluster_id
        self.clusters[cluster_id]['count'] += 1
        self.clusters[cluster_id]['reports'].append(report_idx)

    def _find_matching_cluster(self, text: str) -> Optional[int]:
        """
        æŸ¥æ‰¾ä¸æ–°æ–‡æœ¬æœ€åŒ¹é…çš„èšç±»
        :param text: å¾…åŒ¹é…çš„æ–‡æœ¬
        :return: åŒ¹é…çš„èšç±»IDï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›None
        """
        if not self.clusters or self.tfidf_matrix is None or self.tfidf_matrix.shape[0] == 0:
            return None
        
        try:
            # å°è¯•å‘é‡åŒ–æ–°æ–‡æœ¬ï¼ˆä½¿ç”¨ç°æœ‰vocabularyï¼‰
            new_vector = self.vectorizer.transform([text])
        except:
            # å¦‚æœtransformå¤±è´¥ï¼ˆä¾‹å¦‚vocabularyä¸åŒ…å«æ–°è¯ï¼‰ï¼Œè¿”å›Noneï¼Œè®©è°ƒç”¨è€…é‡å»ºvectorizer
            return None
        
        # è®¡ç®—ä¸æ¯ä¸ªèšç±»ä»£è¡¨æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        best_similarity = 0.0
        best_cluster_id = None
        
        # åªä¸ç°æœ‰èšç±»çš„ä»£è¡¨æ–‡æœ¬æ¯”è¾ƒï¼ˆæé«˜æ•ˆç‡ï¼‰
        for cluster_id, cluster_info in self.clusters.items():
            representative_idx = cluster_info['reports'][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŠ¥å‘Šä½œä¸ºä»£è¡¨
            
            # è·å–ä»£è¡¨æ–‡æœ¬çš„å‘é‡
            if representative_idx < self.tfidf_matrix.shape[0]:
                rep_vector = self.tfidf_matrix[representative_idx:representative_idx+1]
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = cosine_similarity(new_vector, rep_vector)[0][0]
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_cluster_id = cluster_id
        
        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›åŒ¹é…çš„èšç±»ID
        if best_similarity >= self.similarity_threshold:
            return best_cluster_id
        
        return None

    def _rebuild_vectorizer(self):
        """é‡æ–°æ„å»ºå‘é‡åŒ–å™¨ï¼ˆå½“æ·»åŠ æ–°æ–‡æœ¬å¯¼è‡´vocabularyå˜åŒ–æ—¶ï¼‰"""
        if len(self.report_texts) > 0:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.report_texts)

    def find_similar_reports(self, text: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ‰¾å‡ºä¸ç»™å®šæ–‡æœ¬æœ€ç›¸ä¼¼çš„å†å²æŠ¥å‘Š
        :param text: å¾…åŒ¹é…çš„é—®é¢˜æ–‡æœ¬
        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„å‰kä¸ª
        :return: [(ç›¸ä¼¼æŠ¥å‘Š, ç›¸ä¼¼åº¦), ...]ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not self.report_texts or self.tfidf_matrix is None:
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.vectorizer.transform([text])
        
        # è®¡ç®—ä¸æ‰€æœ‰æŠ¥å‘Šçš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [(self.report_texts[i], similarities[i]) for i in top_indices if similarities[i] >= self.similarity_threshold]
        
        return results

    def get_clusters(self) -> Dict[str, Dict]:
        """
        è·å–æ‰€æœ‰èšç±»ä¿¡æ¯
        :return: {èšç±»ID: {representative: ä»£è¡¨æ–‡æœ¬, count: æ•°é‡, reports: [æŠ¥å‘Šåˆ—è¡¨]}}
        """
        result = {}
        for cluster_id, cluster_info in self.clusters.items():
            result[str(cluster_id)] = {
                'representative': cluster_info['representative'],
                'count': cluster_info['count'],
                'reports': [self.report_texts[idx] for idx in cluster_info['reports']]
            }
        return result

    def get_hotspot_ranking(self, top_k: int = 10, now: Optional[datetime] = None) -> List[Tuple[str, float, int, int]]:
        """
        è·å–çƒ­åº¦æ’è¡Œæ¦œï¼ˆæŒ‰è®¡ç®—åçš„çƒ­åº¦é™åºï¼‰
        :param top_k: è¿”å›å‰å¤šå°‘æ¡
        :param now: è®¡ç®—çƒ­åº¦çš„å‚è€ƒæ—¶é—´ï¼Œé»˜è®¤ç°åœ¨
        :return: [(ä»£è¡¨é—®é¢˜æ–‡æœ¬, è®¡ç®—çƒ­åº¦, ä¸ŠæŠ¥æ¬¡æ•°, èšç±»ID), ...]
        """
        if now is None:
            now = datetime.now()
        if not self.clusters:
            return []
        # è®¡ç®—æ¯ä¸ªèšç±»çš„çƒ­åº¦
        ranked = []
        for cluster_id, cluster_info in self.clusters.items():
            heat = self.compute_heat_for_cluster(cluster_id, now=now)
            count = cluster_info.get('count', 0)
            ranked.append((cluster_info['representative'], heat, count, cluster_id))

        ranked_sorted = sorted(ranked, key=lambda x: x[1], reverse=True)
        # è¿”å›å‰top_kä¸ª
        return ranked_sorted[:top_k]


    def print_hotspot(self, top_k: int = 10) -> None:
        """
        æ‰“å°çƒ­åº¦æ’è¡Œï¼Œæ ¼å¼ç±»ä¼¼å¾®åšçƒ­æœ
        """
        ranking = self.get_hotspot_ranking(top_k)
        
        if not ranking:
            print("\næš‚æ— é—®é¢˜æ•°æ®")
            return
        
        print("\nğŸ”¥ å¸‚æ”¿è®¾æ–½é—®é¢˜çƒ­åº¦æ’è¡Œæ¦œ ğŸ”¥")
        print("=" * 60)
        for idx, (issue, heat, count, cluster_id) in enumerate(ranking, start=1):
            # æ·»åŠ çƒ­åº¦æ ‡ç­¾
            if idx == 1:
                tag = "ğŸ”¥"
            elif idx <= 3:
                tag = "â­"
            else:
                tag = "  "
            print(f"{tag} {idx}. {issue}")
            print(f"   çƒ­åº¦: {heat:.2f} | ä¸ŠæŠ¥æ¬¡æ•°: {count} | èšç±»ID: {cluster_id}")
        
        print("=" * 60)

    def get_cluster_reports(self, cluster_id: int) -> List[str]:
        """
        è·å–æŒ‡å®šèšç±»ä¸­çš„æ‰€æœ‰æŠ¥å‘Š
        :param cluster_id: èšç±»ID
        :return: æŠ¥å‘Šåˆ—è¡¨
        """
        if cluster_id not in self.clusters:
            return []
        
        return [self.report_texts[idx] for idx in self.clusters[cluster_id]['reports']]

    def get_statistics(self) -> Dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        :return: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'total_reports': len(self.report_texts),
            'total_clusters': len(self.clusters),
            'avg_reports_per_cluster': len(self.report_texts) / len(self.clusters) if self.clusters else 0
        }

    def load_from_database(self, db_session: 'Session'):
        """
        ä»æ•°æ®åº“åŠ è½½å†å²æŠ¥å‘Šæ•°æ®å¹¶é‡å»ºèšç±»
        :param db_session: æ•°æ®åº“ä¼šè¯
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from model.db import WorkOrderNumberTable
            
            # æŸ¥è¯¢å¾…å—ç†å·¥å•ï¼šæœªå¤„ç†ä¸”æœªå®Œæˆè¯„åˆ†çš„å·¥å•
            # å·²å®Œæˆè¯„åˆ†çš„å·¥å•ï¼ˆwork_form_scoreä¸ä¸ºNoneä¸”ä¸ä¸º0ï¼‰ä¸åº”è¯¥å‡ºç°åœ¨å¾…å—ç†åˆ—è¡¨ä¸­
            user_reports = db_session.query(WorkOrderNumberTable).filter(
                WorkOrderNumberTable.work_content.isnot(None),
                WorkOrderNumberTable.work_content != '',
                WorkOrderNumberTable.work_status == 'æœªå¤„ç†',
                # æ’é™¤å·²å®Œæˆè¯„åˆ†çš„å·¥å•ï¼šwork_form_scoreä¸ºNoneæˆ–0
                ((WorkOrderNumberTable.work_form_score.is_(None)) |
                 (WorkOrderNumberTable.work_form_score == 0.0))
            ).order_by(WorkOrderNumberTable.report_time.desc()).all()
            
            # æ— è®ºæ˜¯å¦æœ‰æ•°æ®ï¼Œéƒ½å…ˆæ¸…ç©ºç°æœ‰æ•°æ®
            self.report_texts = []
            self.report_times = []
            self.report_cluster_map = {}
            self.clusters = {}
            self.cluster_counter = 0
            self.tfidf_matrix = None
            
            if not user_reports:
                print("æ•°æ®åº“ä¸­æ²¡æœ‰å†å²æŠ¥å‘Šæ•°æ®ï¼Œå·²æ¸…ç©ºæ‰€æœ‰èšç±»")
                return
            
            # åŠ è½½æ‰€æœ‰æŠ¥å‘Šå†…å®¹
            print(f"æ­£åœ¨ä»æ•°æ®åº“åŠ è½½ {len(user_reports)} æ¡å†å²æŠ¥å‘Š...")
            for report in user_reports:
                if report.work_content and report.work_content.strip():
                    # å°è¯•è§£æ report.report_time ä¸º datetime
                    rt = report.report_time
                    if isinstance(rt, str):
                        try:
                            parsed = datetime.fromisoformat(rt.replace('Z', '+00:00'))
                        except:
                            parsed = datetime.now()
                    elif isinstance(rt, datetime):
                        parsed = rt
                    else:
                        parsed = datetime.now()

                    # ä½¿ç”¨add_reportæ–¹æ³•æ·»åŠ ï¼Œä¼šè‡ªåŠ¨è¿›è¡Œèšç±»ï¼Œå¹¶ä¿å­˜æ—¶é—´
                    self.add_report(report.work_content.strip(), report_time=parsed)
            
            print(f"æˆåŠŸåŠ è½½ {len(self.report_texts)} æ¡æŠ¥å‘Šï¼Œå½¢æˆ {len(self.clusters)} ä¸ªèšç±»")
        except Exception as e:
            print(e)

    def reload_from_database(self, db_session: 'Session'):
        """
        é‡æ–°ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼ˆç”¨äºåˆ·æ–°ï¼‰
        :param db_session: æ•°æ®åº“ä¼šè¯
        """
        self.load_from_database(db_session)