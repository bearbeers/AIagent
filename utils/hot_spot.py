from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import jieba
from typing import List, Dict, Tuple, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from sqlalchemy.orm import Session


class MunicipalHotspotRanker:
    """
    å¸‚æ”¿åŸºç¡€è®¾æ–½é—®é¢˜çƒ­åº¦åˆ†æå™¨
    åŠŸèƒ½ï¼šè¯†åˆ«ç›¸ä¼¼é—®é¢˜ã€èšç±»å½’é›†ã€ç”Ÿæˆçƒ­åº¦æ’è¡Œæ¦œ
    """
    
    def __init__(self, similarity_threshold: float = 0.6, db_session: Optional['Session'] = None):
        """
        åˆå§‹åŒ–çƒ­åº¦åˆ†æå™¨
        :param similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œå€¼è¶Šå¤§è¦æ±‚è¶Šç›¸ä¼¼
        :param db_session: æ•°æ®åº“ä¼šè¯ï¼Œå¦‚æœæä¾›åˆ™ä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®
        """
        self.reports = []  # å­˜å‚¨æ‰€æœ‰ä¸ŠæŠ¥çš„é—®é¢˜
        self.report_cluster_map = {}  # æŠ¥å‘Šç´¢å¼•åˆ°èšç±»IDçš„æ˜ å°„
        self.clusters = {}  # èšç±»ä¿¡æ¯ {cluster_id: {'representative': ä»£è¡¨æ–‡æœ¬, 'count': æ•°é‡, 'reports': [ç´¢å¼•åˆ—è¡¨]}}
        self.cluster_counter = 0  # èšç±»IDè®¡æ•°å™¨
        
        # ä¸­æ–‡åˆ†è¯å™¨é…ç½®
        self.tokenizer = lambda text: ' '.join(jieba.cut(text))
        
        # TF-IDFå‘é‡åŒ–å™¨ï¼Œæ”¯æŒä¸­æ–‡
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.tokenizer,
            token_pattern=None,
            lowercase=False,
            max_features=5000,
            ngram_range=(1, 2)  # æ”¯æŒ1-gramå’Œ2-gram
        )
        self.tfidf_matrix = None
        self.similarity_threshold = similarity_threshold
        
        # å¦‚æœæä¾›äº†æ•°æ®åº“ä¼šè¯ï¼Œä»æ•°æ®åº“åŠ è½½å†å²æ•°æ®
        if db_session is not None:
            self.load_from_database(db_session)

    def add_report(self, text: str) -> int:
        """
        æ·»åŠ ä¸€æ¡æ–°çš„å¸‚æ”¿é—®é¢˜ä¸ŠæŠ¥è®°å½•ï¼Œå¹¶è‡ªåŠ¨è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…å’Œèšç±»
        :param text: ç”¨æˆ·ä¸ŠæŠ¥çš„é—®é¢˜æ–‡æœ¬
        :return: æ–°æŠ¥å‘Šçš„ç´¢å¼•
        """
        if not text or not text.strip():
            raise ValueError("é—®é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        text = text.strip()
        report_idx = len(self.reports)
        self.reports.append(text)
        
        # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ¡æŠ¥å‘Šï¼Œç›´æ¥åˆ›å»ºæ–°èšç±»
        if report_idx == 0:
            self._create_new_cluster(report_idx, text)
            self.tfidf_matrix = self.vectorizer.fit_transform([text])
        else:
            # å…ˆä¸´æ—¶å‘é‡åŒ–æ–°æ–‡æœ¬ï¼ˆä½¿ç”¨å·²æœ‰vocabularyï¼‰ï¼Œç”¨äºç›¸ä¼¼åº¦åŒ¹é…
            # å¦‚æœvectorizerè¿˜æœªfitï¼Œåˆ™å…ˆfitæ‰€æœ‰å·²æœ‰æŠ¥å‘Š
            if self.tfidf_matrix is None or self.tfidf_matrix.shape[0] == 0:
                self._rebuild_vectorizer()
            
            # å°è¯•åŒ¹é…åˆ°ç°æœ‰èšç±»
            matched_cluster_id = self._find_matching_cluster(text)
            
            if matched_cluster_id is None:
                # æ²¡æœ‰åŒ¹é…åˆ°ï¼Œåˆ›å»ºæ–°èšç±»
                self._create_new_cluster(report_idx, text)
            else:
                # åŒ¹é…åˆ°ç°æœ‰èšç±»ï¼Œæ·»åŠ åˆ°è¯¥èšç±»
                self._add_to_cluster(report_idx, matched_cluster_id)
            
            # é‡æ–°æ„å»ºå‘é‡çŸ©é˜µï¼ˆç¡®ä¿vocabularyåŒ…å«æ‰€æœ‰æ–°è¯ï¼‰
            self._rebuild_vectorizer()
        
        return report_idx

    def _create_new_cluster(self, report_idx: int, text: str) -> int:
        """åˆ›å»ºæ–°çš„èšç±»"""
        cluster_id = self.cluster_counter
        self.cluster_counter += 1
        self.report_cluster_map[report_idx] = cluster_id
        self.clusters[cluster_id] = {
            'representative': text,
            'count': 1,
            'reports': [report_idx]
        }
        return cluster_id

    def _add_to_cluster(self, report_idx: int, cluster_id: int):
        """å°†æŠ¥å‘Šæ·»åŠ åˆ°æŒ‡å®šèšç±»"""
        self.report_cluster_map[report_idx] = cluster_id
        self.clusters[cluster_id]['count'] += 1
        self.clusters[cluster_id]['reports'].append(report_idx)

    def _find_matching_cluster(self, text: str) -> Optional[int]:
        """
        æŸ¥æ‰¾ä¸æ–°æ–‡æœ¬æœ€åŒ¹é…çš„èšç±»
        :param text: å¾…åŒ¹é…çš„æ–‡æœ¬
        :return: åŒ¹é…çš„èšç±»IDï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›None
        """
        if not self.clusters or self.tfidf_matrix is None or self.tfidf_matrix.shape[0] == 0:
            return None
        
        try:
            # å°è¯•å‘é‡åŒ–æ–°æ–‡æœ¬ï¼ˆä½¿ç”¨ç°æœ‰vocabularyï¼‰
            new_vector = self.vectorizer.transform([text])
        except:
            # å¦‚æœtransformå¤±è´¥ï¼ˆä¾‹å¦‚vocabularyä¸åŒ…å«æ–°è¯ï¼‰ï¼Œè¿”å›Noneï¼Œè®©è°ƒç”¨è€…é‡å»ºvectorizer
            return None
        
        # è®¡ç®—ä¸æ¯ä¸ªèšç±»ä»£è¡¨æ–‡æœ¬çš„ç›¸ä¼¼åº¦
        best_similarity = 0.0
        best_cluster_id = None
        
        # åªä¸ç°æœ‰èšç±»çš„ä»£è¡¨æ–‡æœ¬æ¯”è¾ƒï¼ˆæé«˜æ•ˆç‡ï¼‰
        for cluster_id, cluster_info in self.clusters.items():
            representative_idx = cluster_info['reports'][0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæŠ¥å‘Šä½œä¸ºä»£è¡¨
            
            # è·å–ä»£è¡¨æ–‡æœ¬çš„å‘é‡
            if representative_idx < self.tfidf_matrix.shape[0]:
                rep_vector = self.tfidf_matrix[representative_idx:representative_idx+1]
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = cosine_similarity(new_vector, rep_vector)[0][0]
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_cluster_id = cluster_id
        
        # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›åŒ¹é…çš„èšç±»ID
        if best_similarity >= self.similarity_threshold:
            return best_cluster_id
        
        return None

    def _rebuild_vectorizer(self):
        """é‡æ–°æ„å»ºå‘é‡åŒ–å™¨ï¼ˆå½“æ·»åŠ æ–°æ–‡æœ¬å¯¼è‡´vocabularyå˜åŒ–æ—¶ï¼‰"""
        if len(self.reports) > 0:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.reports)

    def find_similar_reports(self, text: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        æ‰¾å‡ºä¸ç»™å®šæ–‡æœ¬æœ€ç›¸ä¼¼çš„å†å²æŠ¥å‘Š
        :param text: å¾…åŒ¹é…çš„é—®é¢˜æ–‡æœ¬
        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„å‰kä¸ª
        :return: [(ç›¸ä¼¼æŠ¥å‘Š, ç›¸ä¼¼åº¦), ...]ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        if not self.reports or self.tfidf_matrix is None:
            return []
        
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        query_vector = self.vectorizer.transform([text])
        
        # è®¡ç®—ä¸æ‰€æœ‰æŠ¥å‘Šçš„ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.tfidf_matrix)[0]
        
        # è·å–top_kä¸ªæœ€ç›¸ä¼¼çš„ç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        results = [(self.reports[i], similarities[i]) for i in top_indices if similarities[i] >= self.similarity_threshold]
        
        return results

    def get_clusters(self) -> Dict[str, Dict]:
        """
        è·å–æ‰€æœ‰èšç±»ä¿¡æ¯
        :return: {èšç±»ID: {representative: ä»£è¡¨æ–‡æœ¬, count: æ•°é‡, reports: [æŠ¥å‘Šåˆ—è¡¨]}}
        """
        result = {}
        for cluster_id, cluster_info in self.clusters.items():
            result[str(cluster_id)] = {
                'representative': cluster_info['representative'],
                'count': cluster_info['count'],
                'reports': [self.reports[idx] for idx in cluster_info['reports']]
            }
        return result

    def get_hotspot_ranking(self, top_k: int = 10) -> List[Tuple[str, int, int]]:
        """
        è·å–çƒ­åº¦æ’è¡Œæ¦œï¼ˆæŒ‰æ•°é‡é™åºï¼‰
        :param top_k: è¿”å›å‰å¤šå°‘æ¡
        :return: [(ä»£è¡¨é—®é¢˜æ–‡æœ¬, æ•°é‡, èšç±»ID), ...]
        """
        if not self.clusters:
            return []
        
        # æŒ‰æ•°é‡æ’åº
        ranked = sorted(
            self.clusters.items(),
            key=lambda x: x[1]['count'],
            reverse=True
        )
        
        # è¿”å›å‰top_kä¸ª
        result = [
            (cluster_info['representative'], cluster_info['count'], cluster_id)
            for cluster_id, cluster_info in ranked[:top_k]
        ]
        
        return result

    def print_hotspot(self, top_k: int = 10) -> None:
        """
        æ‰“å°çƒ­åº¦æ’è¡Œï¼Œæ ¼å¼ç±»ä¼¼å¾®åšçƒ­æœ
        """
        ranking = self.get_hotspot_ranking(top_k)
        
        if not ranking:
            print("\næš‚æ— é—®é¢˜æ•°æ®")
            return
        
        print("\nğŸ”¥ å¸‚æ”¿è®¾æ–½é—®é¢˜çƒ­åº¦æ’è¡Œæ¦œ ğŸ”¥")
        print("=" * 60)
        for idx, (issue, count, cluster_id) in enumerate(ranking, start=1):
            # æ·»åŠ çƒ­åº¦æ ‡ç­¾
            if idx == 1:
                tag = "ğŸ”¥"
            elif idx <= 3:
                tag = "â­"
            else:
                tag = "  "
            print(f"{tag} {idx}. {issue}")
            print(f"   çƒ­åº¦: {count} | èšç±»ID: {cluster_id}")
        
        print("=" * 60)

    def get_cluster_reports(self, cluster_id: int) -> List[str]:
        """
        è·å–æŒ‡å®šèšç±»ä¸­çš„æ‰€æœ‰æŠ¥å‘Š
        :param cluster_id: èšç±»ID
        :return: æŠ¥å‘Šåˆ—è¡¨
        """
        if cluster_id not in self.clusters:
            return []
        
        return [self.reports[idx] for idx in self.clusters[cluster_id]['reports']]

    def get_statistics(self) -> Dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯
        :return: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'total_reports': len(self.reports),
            'total_clusters': len(self.clusters),
            'avg_reports_per_cluster': len(self.reports) / len(self.clusters) if self.clusters else 0
        }

    def load_from_database(self, db_session: 'Session'):
        """
        ä»æ•°æ®åº“åŠ è½½å†å²æŠ¥å‘Šæ•°æ®å¹¶é‡å»ºèšç±»
        :param db_session: æ•°æ®åº“ä¼šè¯
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from model.db import WorkOrderNumberTable
            
            # æŸ¥è¯¢å¾…å—ç†å·¥å•ï¼šæœªå¤„ç†ä¸”æœªå®Œæˆè¯„åˆ†çš„å·¥å•
            # å·²å®Œæˆè¯„åˆ†çš„å·¥å•ï¼ˆwork_form_scoreä¸ä¸ºNoneä¸”ä¸ä¸º0ï¼‰ä¸åº”è¯¥å‡ºç°åœ¨å¾…å—ç†åˆ—è¡¨ä¸­
            user_reports = db_session.query(WorkOrderNumberTable).filter(
                WorkOrderNumberTable.work_content.isnot(None),
                WorkOrderNumberTable.work_content != '',
                WorkOrderNumberTable.work_status == 'æœªå¤„ç†',
                # æ’é™¤å·²å®Œæˆè¯„åˆ†çš„å·¥å•ï¼šwork_form_scoreä¸ºNoneæˆ–0
                ((WorkOrderNumberTable.work_form_score.is_(None)) |
                 (WorkOrderNumberTable.work_form_score == 0.0))
            ).order_by(WorkOrderNumberTable.report_time.desc()).all()
            
            # æ— è®ºæ˜¯å¦æœ‰æ•°æ®ï¼Œéƒ½å…ˆæ¸…ç©ºç°æœ‰æ•°æ®
            self.reports = []
            self.report_cluster_map = {}
            self.clusters = {}
            self.cluster_counter = 0
            self.tfidf_matrix = None
            
            if not user_reports:
                print("æ•°æ®åº“ä¸­æ²¡æœ‰å†å²æŠ¥å‘Šæ•°æ®ï¼Œå·²æ¸…ç©ºæ‰€æœ‰èšç±»")
                return
            
            # åŠ è½½æ‰€æœ‰æŠ¥å‘Šå†…å®¹
            print(f"æ­£åœ¨ä»æ•°æ®åº“åŠ è½½ {len(user_reports)} æ¡å†å²æŠ¥å‘Š...")
            for report in user_reports:
                if report.work_content and report.work_content.strip():
                    # ä½¿ç”¨add_reportæ–¹æ³•æ·»åŠ ï¼Œä¼šè‡ªåŠ¨è¿›è¡Œèšç±»
                    self.add_report(report.work_content.strip())
            
            print(f"æˆåŠŸåŠ è½½ {len(self.reports)} æ¡æŠ¥å‘Šï¼Œå½¢æˆ {len(self.clusters)} ä¸ªèšç±»")
            
        except Exception as e:
            print(f"ä»æ•°æ®åº“åŠ è½½æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def reload_from_database(self, db_session: 'Session'):
        """
        é‡æ–°ä»æ•°æ®åº“åŠ è½½æ•°æ®ï¼ˆç”¨äºåˆ·æ–°ï¼‰
        :param db_session: æ•°æ®åº“ä¼šè¯
        """
        self.load_from_database(db_session)


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # åˆ›å»ºçƒ­åº¦åˆ†æå™¨ï¼Œè®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º0.6
    ranker = MunicipalHotspotRanker(similarity_threshold=0.6)

    # æ¨¡æ‹Ÿç”¨æˆ·éšæ—¶ä¸ŠæŠ¥çš„å¸‚æ”¿åŸºç¡€è®¾æ–½é—®é¢˜
    reports = [
        "æ°´ç®¡çˆ†è£‚ä¸¥é‡ï¼Œéœ€è¦é©¬ä¸Šå¤„ç†",
        "æ°´ç®¡çˆ†è£‚,å¯¼è‡´å¤§é¢ç§¯åœæ°´,è¯·å°½å¿«å¤„ç†",
        "è‡ªæ¥æ°´ç®¡é“æ¼æ°´ï¼Œå½±å“å±…æ°‘ç”¨æ°´",
        "ç‡ƒæ°”æ³„æ¼é£é™©ï¼Œéœ€è¦é©¬ä¸Šå»å¤„ç†",
        "ç‡ƒæ°”ç®¡é“æœ‰å¼‚å‘³ï¼Œç–‘ä¼¼æ³„æ¼",
        "ä¸‹æ°´é“å µå¡ï¼Œæ±¡æ°´å¤–æº¢",
        "æ±¡æ°´äº•ç›–ç ´æŸï¼Œå­˜åœ¨å®‰å…¨éšæ‚£",
        "ä¾›æ°´ä¸»ç®¡é“çˆ†è£‚ï¼Œå¯¼è‡´å¤§é¢ç§¯åœæ°´",
        "ç‡ƒæ°”æŠ¥è­¦å™¨å“èµ·ï¼Œæ€€ç–‘æ³„æ¼",
        "è·¯ç¯ä¸äº®ï¼Œå½±å“å¤œé—´å‡ºè¡Œå®‰å…¨",
        "é“è·¯ç…§æ˜ç¯æ•…éšœï¼Œéœ€è¦ç»´ä¿®",
        "äº¤é€šä¿¡å·ç¯ä¸å·¥ä½œï¼Œå½±å“äº¤é€š",
        "åƒåœ¾æ¡¶æ»¡äº†ï¼Œéœ€è¦æ¸…ç†",
        "åƒåœ¾ç®±æº¢å‡ºï¼Œå¼‚å‘³ä¸¥é‡",
    ]

    print("æ­£åœ¨å¤„ç†ä¸ŠæŠ¥çš„é—®é¢˜...")
    for i, r in enumerate(reports, 1):
        ranker.add_report(r)
        print(f"å·²å¤„ç†ç¬¬ {i} æ¡ä¸ŠæŠ¥")

    # æ‰“å°çƒ­åº¦æ’è¡Œ
    ranker.print_hotspot()

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    stats = ranker.get_statistics()
    print(f"æ€»ä¸ŠæŠ¥æ•°: {stats['total_reports']}")
    print(f"èšç±»æ•°é‡: {stats['total_clusters']}")
    print(f"å¹³å‡æ¯ç±»é—®é¢˜æ•°: {stats['avg_reports_per_cluster']:.2f}")

    # æŸ¥çœ‹ç›¸ä¼¼é—®é¢˜ç¤ºä¾‹
    print("\nğŸ” æŸ¥æ‰¾ç›¸ä¼¼é—®é¢˜ç¤ºä¾‹:")
    query = "æ°´ç®¡çˆ†è£‚"
    similar = ranker.find_similar_reports(query, top_k=3)
    print(f"æŸ¥è¯¢: '{query}'")
    for report, similarity in similar:
        print(f"  - {report} (ç›¸ä¼¼åº¦: {similarity:.2f})")
